{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model \n",
    "from sklearn import svm\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import warnings\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "names = [\n",
    "                    \"PassengerId\", \n",
    "                     \"Survived\",\n",
    "                     \"Pclass\",\n",
    "                     \"Name\",\n",
    "                     \"Sex\",\n",
    "                     \"Age\",\n",
    "                     \"SibSp\",\n",
    "                     \"Parch\",\n",
    "                     \"Ticket\",\n",
    "                     \"Fare\",\n",
    "                     \"Cabin\",\n",
    "                     \"Embarked\"\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"train.csv\",na_values=\"?\")\n",
    "df1 = pd.read_csv(\"test.csv\",na_values=\"?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#drop all irrelevant data\n",
    "df = df.dropna(axis=0)\n",
    "df1 = df1.dropna(axis=0)\n",
    "#drop all incompatible data types\n",
    "#to-do: map all 'object' data types to numerical values\n",
    "df = df.select_dtypes(exclude=[\"object\"])\n",
    "df1 = df1.select_dtypes(exclude=['object'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId  Survived  Pclass   Age  SibSp  Parch     Fare\n",
       "1             2         1       1  38.0      1      0  71.2833\n",
       "3             4         1       1  35.0      1      0  53.1000\n",
       "6             7         0       1  54.0      0      0  51.8625\n",
       "10           11         1       3   4.0      1      1  16.7000\n",
       "11           12         1       1  58.0      0      0  26.5500\n",
       "21           22         1       2  34.0      0      0  13.0000"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>904</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>82.2667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>906</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>61.1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>916</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>262.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>918</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.9792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>920</td>\n",
       "      <td>1</td>\n",
       "      <td>41.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>926</td>\n",
       "      <td>1</td>\n",
       "      <td>30.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.7500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    PassengerId  Pclass   Age  SibSp  Parch      Fare\n",
       "12          904       1  23.0      1      0   82.2667\n",
       "14          906       1  47.0      1      0   61.1750\n",
       "24          916       1  48.0      1      3  262.3750\n",
       "26          918       1  22.0      0      1   61.9792\n",
       "28          920       1  41.0      0      0   30.5000\n",
       "34          926       1  30.0      1      0   57.7500"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples=df.shape[0]\n",
    "num_attributes = df.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The titanic dataset contains 183 observations\n",
      "The titanic dataset contains 7 attributes\n"
     ]
    }
   ],
   "source": [
    "print (\"The titanic dataset contains {} observations\".format(num_samples))\n",
    "print (\"The titanic dataset contains {} attributes\".format(num_attributes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>22</td>\n",
       "      <td>2</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>24</td>\n",
       "      <td>1</td>\n",
       "      <td>28.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>35.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>53</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>76.7292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>55</td>\n",
       "      <td>1</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>61.9792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>45.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>83.4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>67</td>\n",
       "      <td>2</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>76</td>\n",
       "      <td>3</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.6500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>89</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>263.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>93</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>61.1750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>97</td>\n",
       "      <td>1</td>\n",
       "      <td>71.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>34.6542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>98</td>\n",
       "      <td>1</td>\n",
       "      <td>23.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>63.3583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>103</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>77.2875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110</th>\n",
       "      <td>111</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>118</th>\n",
       "      <td>119</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>247.5208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>124</td>\n",
       "      <td>2</td>\n",
       "      <td>32.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>125</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>77.2875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>137</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26.2833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>138</td>\n",
       "      <td>1</td>\n",
       "      <td>37.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>140</td>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>149</td>\n",
       "      <td>2</td>\n",
       "      <td>36.5</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>26.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>152</td>\n",
       "      <td>1</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>66.6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>171</td>\n",
       "      <td>1</td>\n",
       "      <td>61.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>33.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174</th>\n",
       "      <td>175</td>\n",
       "      <td>1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.6958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177</th>\n",
       "      <td>178</td>\n",
       "      <td>1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>28.7125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>737</th>\n",
       "      <td>738</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>512.3292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>741</th>\n",
       "      <td>742</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>78.8500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>742</th>\n",
       "      <td>743</td>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>262.3750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745</th>\n",
       "      <td>746</td>\n",
       "      <td>1</td>\n",
       "      <td>70.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>748</th>\n",
       "      <td>749</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>751</th>\n",
       "      <td>752</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>759</th>\n",
       "      <td>760</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>764</td>\n",
       "      <td>1</td>\n",
       "      <td>36.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>765</th>\n",
       "      <td>766</td>\n",
       "      <td>1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>77.9583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>773</td>\n",
       "      <td>2</td>\n",
       "      <td>57.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>779</th>\n",
       "      <td>780</td>\n",
       "      <td>1</td>\n",
       "      <td>43.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>211.3375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>781</th>\n",
       "      <td>782</td>\n",
       "      <td>1</td>\n",
       "      <td>17.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>57.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>782</th>\n",
       "      <td>783</td>\n",
       "      <td>1</td>\n",
       "      <td>29.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>789</th>\n",
       "      <td>790</td>\n",
       "      <td>1</td>\n",
       "      <td>46.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>79.2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>796</th>\n",
       "      <td>797</td>\n",
       "      <td>1</td>\n",
       "      <td>49.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.9292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>802</th>\n",
       "      <td>803</td>\n",
       "      <td>1</td>\n",
       "      <td>11.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>120.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>807</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>809</th>\n",
       "      <td>810</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>820</th>\n",
       "      <td>821</td>\n",
       "      <td>1</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>93.5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>823</th>\n",
       "      <td>824</td>\n",
       "      <td>3</td>\n",
       "      <td>27.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>12.4750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>835</th>\n",
       "      <td>836</td>\n",
       "      <td>1</td>\n",
       "      <td>39.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>853</th>\n",
       "      <td>854</td>\n",
       "      <td>1</td>\n",
       "      <td>16.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>39.4000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>857</th>\n",
       "      <td>858</td>\n",
       "      <td>1</td>\n",
       "      <td>51.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>862</th>\n",
       "      <td>863</td>\n",
       "      <td>1</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25.9292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>867</th>\n",
       "      <td>868</td>\n",
       "      <td>1</td>\n",
       "      <td>31.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50.4958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>871</th>\n",
       "      <td>872</td>\n",
       "      <td>1</td>\n",
       "      <td>47.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>52.5542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>872</th>\n",
       "      <td>873</td>\n",
       "      <td>1</td>\n",
       "      <td>33.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>880</td>\n",
       "      <td>1</td>\n",
       "      <td>56.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>83.1583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>888</td>\n",
       "      <td>1</td>\n",
       "      <td>19.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>890</td>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>30.0000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>183 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     PassengerId  Pclass   Age  SibSp  Parch      Fare\n",
       "1              2       1  38.0      1      0   71.2833\n",
       "3              4       1  35.0      1      0   53.1000\n",
       "6              7       1  54.0      0      0   51.8625\n",
       "10            11       3   4.0      1      1   16.7000\n",
       "11            12       1  58.0      0      0   26.5500\n",
       "21            22       2  34.0      0      0   13.0000\n",
       "23            24       1  28.0      0      0   35.5000\n",
       "27            28       1  19.0      3      2  263.0000\n",
       "52            53       1  49.0      1      0   76.7292\n",
       "54            55       1  65.0      0      1   61.9792\n",
       "62            63       1  45.0      1      0   83.4750\n",
       "66            67       2  29.0      0      0   10.5000\n",
       "75            76       3  25.0      0      0    7.6500\n",
       "88            89       1  23.0      3      2  263.0000\n",
       "92            93       1  46.0      1      0   61.1750\n",
       "96            97       1  71.0      0      0   34.6542\n",
       "97            98       1  23.0      0      1   63.3583\n",
       "102          103       1  21.0      0      1   77.2875\n",
       "110          111       1  47.0      0      0   52.0000\n",
       "118          119       1  24.0      0      1  247.5208\n",
       "123          124       2  32.5      0      0   13.0000\n",
       "124          125       1  54.0      0      1   77.2875\n",
       "136          137       1  19.0      0      2   26.2833\n",
       "137          138       1  37.0      1      0   53.1000\n",
       "139          140       1  24.0      0      0   79.2000\n",
       "148          149       2  36.5      0      2   26.0000\n",
       "151          152       1  22.0      1      0   66.6000\n",
       "170          171       1  61.0      0      0   33.5000\n",
       "174          175       1  56.0      0      0   30.6958\n",
       "177          178       1  50.0      0      0   28.7125\n",
       "..           ...     ...   ...    ...    ...       ...\n",
       "737          738       1  35.0      0      0  512.3292\n",
       "741          742       1  36.0      1      0   78.8500\n",
       "742          743       1  21.0      2      2  262.3750\n",
       "745          746       1  70.0      1      1   71.0000\n",
       "748          749       1  19.0      1      0   53.1000\n",
       "751          752       3   6.0      0      1   12.4750\n",
       "759          760       1  33.0      0      0   86.5000\n",
       "763          764       1  36.0      1      2  120.0000\n",
       "765          766       1  51.0      1      0   77.9583\n",
       "772          773       2  57.0      0      0   10.5000\n",
       "779          780       1  43.0      0      1  211.3375\n",
       "781          782       1  17.0      1      0   57.0000\n",
       "782          783       1  29.0      0      0   30.0000\n",
       "789          790       1  46.0      0      0   79.2000\n",
       "796          797       1  49.0      0      0   25.9292\n",
       "802          803       1  11.0      1      2  120.0000\n",
       "806          807       1  39.0      0      0    0.0000\n",
       "809          810       1  33.0      1      0   53.1000\n",
       "820          821       1  52.0      1      1   93.5000\n",
       "823          824       3  27.0      0      1   12.4750\n",
       "835          836       1  39.0      1      1   83.1583\n",
       "853          854       1  16.0      0      1   39.4000\n",
       "857          858       1  51.0      0      0   26.5500\n",
       "862          863       1  48.0      0      0   25.9292\n",
       "867          868       1  31.0      0      0   50.4958\n",
       "871          872       1  47.0      1      1   52.5542\n",
       "872          873       1  33.0      0      0    5.0000\n",
       "879          880       1  56.0      0      1   83.1583\n",
       "887          888       1  19.0      0      0   30.0000\n",
       "889          890       1  26.0      0      0   30.0000\n",
       "\n",
       "[183 rows x 6 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#get the targets for the trainning set \n",
    "#drop the targets for the trainning set\n",
    "Y_train = df[\"Survived\"]\n",
    "df.drop(['Survived'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#returns a numpy.ndarray\n",
    "X_train = df.values\n",
    "X_test = df1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (183, 7)\n",
      "Training Targets shape: (183,)\n",
      "Testing date shape: (183, 7)\n"
     ]
    }
   ],
   "source": [
    "print (\"Training data shape: {}\".format(X_train.shape))\n",
    "print (\"Training Targets shape: {}\".format(Y_train.shape))\n",
    "print (\"Testing date shape: {}\".format(X_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_train_logreg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc_test_logreg = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_logreg = [0.1,1,10,100,1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logreg_model(c, X_train, Y_train,penalty):\n",
    "    '''\n",
    "        Author: Kyle Ong\n",
    "        Date: 05/13/2018\n",
    "        \n",
    "        fits a sklearn.linear_model.LogisticRegression(penalty = penalty, C=c, solver='saga')\n",
    "        to X_train and Y_train\n",
    "        \n",
    "        c: type: float\n",
    "        penalty: type: string\n",
    "        X_train: type: numpy.ndarray\n",
    "        Y_train: type: numpy.ndarray\n",
    "    \n",
    "    '''\n",
    "    logreg = linear_model.LogisticRegression(penalty=penalty, C=c ,solver='saga')\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        \n",
    "        warnings.filterwarnings('ignore',category=ConvergenceWarning)\n",
    "        \n",
    "        try:\n",
    "            logreg.fit(X_train,Y_train)\n",
    "            \n",
    "            y_hat = logreg.predict(X_train)\n",
    "            acc_train = np.mean(Y_train == y_hat)\n",
    "            print(\"This is the trainning accuracy with {} loss and {} c_val: {}\".format(penalty,c, acc_train))\n",
    "            \n",
    "        except Warning as w:\n",
    "            print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the trainning accuracy with L1 loss and 100 c_val: 0.7213114754098361\n"
     ]
    }
   ],
   "source": [
    "logreg_model(100,X_train,Y_train,\"L1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_logistic_model_with(X_train,Y_train,params):\n",
    "    '''\n",
    "            Author: Kyle Ong\n",
    "            Date: 05/12/2018\n",
    "            \n",
    "            will train sklearn.linear_model.LogisticRegression(penalty = penalty, C=c_val,solver='saga) over params\n",
    "            \n",
    "            X_train: type: numpy.ndarray\n",
    "            Y_train: type: numpy.ndarray\n",
    "            params: type: dict: {penalty:[c_vals]}\n",
    "    '''\n",
    "    for k,v in params.items():\n",
    "        \n",
    "        penalty = k\n",
    "        acc_train_array = []\n",
    "\n",
    "        for c_val in v:\n",
    "            #this is baest coding skills\n",
    "            log_model = 1\n",
    "            if penalty.lower() == \"l1\" :\n",
    "                log_model = linear_model.LogisticRegression(penalty=penalty, C=c_val,solver='saga')\n",
    "            \n",
    "            elif penalty.lower() == 'l2':\n",
    "                log_model = linear_model.LogisticRegression( C=c_val)\n",
    "                \n",
    "            with warnings.catch_warnings():\n",
    "                \n",
    "                warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "                \n",
    "                try:\n",
    "                     #fit the model\n",
    "                    log_model.fit(X_train,Y_train)  \n",
    "                    #train the model\n",
    "                    Y_hat = log_model.predict(X_train)\n",
    "            \n",
    "                    train_acc = np.mean(Y_hat == Y_train)\n",
    "            \n",
    "                    acc_train_array.append(train_acc)\n",
    "                    print(\"Accuracy with {} loss and c_val {} : {}\".format(penalty,c_val,train_acc))\n",
    "                \n",
    "                except Warning as w:\n",
    "                    print (w)\n",
    "            \n",
    "\n",
    "      \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"L1\":[0.1,1,10,100],\n",
    "    \"L2\":[0.1,1,10,100]\n",
    "         }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with L1 loss and c_val 0.1 : 0.7213114754098361\n",
      "Accuracy with L1 loss and c_val 1 : 0.7213114754098361\n",
      "Accuracy with L1 loss and c_val 10 : 0.7213114754098361\n",
      "Accuracy with L1 loss and c_val 100 : 0.7213114754098361\n",
      "Accuracy with L2 loss and c_val 0.1 : 0.9726775956284153\n",
      "Accuracy with L2 loss and c_val 1 : 1.0\n",
      "Accuracy with L2 loss and c_val 10 : 1.0\n",
      "Accuracy with L2 loss and c_val 100 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_logistic_model_with(X_train,Y_train,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# apply feature transformations!\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_transformed = poly.fit_transform(X_train)\n",
    "labels = X_train_transformed.shape[1]\n",
    "observations = X_train_transformed.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The titanic dataset has 183 observations.\n",
      "The titanic dataset has 36 labels\n"
     ]
    }
   ],
   "source": [
    "print (\"The titanic dataset has {} observations.\".format(observations))\n",
    "print(\"The titanic dataset has {} labels\".format(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with L1 loss and c_val 0.1 : 0.6939890710382514\n",
      "Accuracy with L1 loss and c_val 1 : 0.6994535519125683\n",
      "Accuracy with L1 loss and c_val 10 : 0.6994535519125683\n",
      "Accuracy with L1 loss and c_val 100 : 0.6939890710382514\n",
      "Accuracy with L2 loss and c_val 0.1 : 1.0\n",
      "Accuracy with L2 loss and c_val 1 : 1.0\n",
      "Accuracy with L2 loss and c_val 10 : 1.0\n",
      "Accuracy with L2 loss and c_val 100 : 1.0\n"
     ]
    }
   ],
   "source": [
    "train_logistic_model_with(X_train_transformed,Y_train,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#split the trainning set into test and training set\n",
    "split = math.floor(num_samples*0.5)\n",
    "\n",
    "X_train_split = X_train[0 : split, :]\n",
    "X_test_split = X_train[split: , :  ]\n",
    "Y_train_split = Y_train[0:split]\n",
    "Y_test_split = Y_train[split: ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training observations has shape: (91, 7)\n",
      "Training targets has shape: (91,)\n",
      "Testing observations has shape: (92, 7)\n"
     ]
    }
   ],
   "source": [
    "print(\"Training observations has shape: {}\".format(X_train_split.shape))\n",
    "print(\"Training targets has shape: {}\".format(Y_train_split.shape))\n",
    "print(\"Testing observations has shape: {}\".format(X_test_split.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test_logistic_model_with(X_train,Y_train,X_test,Y_test,params):\n",
    "    '''\n",
    "            Author: Kyle Ong\n",
    "            Date 05/13/2018\n",
    "            \n",
    "            trains and tests linear_model.LogisticRegression()\n",
    "            \n",
    "            X_train:type: numpy.ndarray\n",
    "            Y_train: type: numpy.ndarray\n",
    "            X_test: type: numpy.ndarray\n",
    "            Y_test: type: numpy.ndarray\n",
    "            params: type: dict {penalty : [c_val]}\n",
    "    '''\n",
    "    for k,v in params.items():\n",
    "        \n",
    "        penalty = k\n",
    "        acc_train_array = []\n",
    "        acc_test_arr = []\n",
    "\n",
    "        for c_val in v:\n",
    "            #this is baest coding skills\n",
    "            log_model = 1\n",
    "            if penalty.lower() == \"l1\" :\n",
    "                log_model = linear_model.LogisticRegression(penalty=penalty, C=c_val,solver='saga')\n",
    "            \n",
    "            elif penalty.lower() == 'l2':\n",
    "                log_model = linear_model.LogisticRegression( C=c_val)\n",
    "               \n",
    "            #uncoment context manager in order to get the convergence warning!\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "                try:\n",
    "                     #fit the model\n",
    "                    log_model.fit(X_train,Y_train)  \n",
    "                \n",
    "                    #train the model\n",
    "                    Y_hat = log_model.predict(X_train)\n",
    "                    train_acc = np.mean(Y_hat == Y_train)\n",
    "                    acc_train_array.append(train_acc)\n",
    "                    print(\"Training() ~> Accuracy with {} loss and c_val {} : \\n{}\".format(penalty,c_val,train_acc))\n",
    "                \n",
    "                    #test the model\n",
    "                    Y_hat_test = log_model.predict(X_test)\n",
    "                    acc_test =np.mean(Y_test == Y_hat_test)\n",
    "                    acc_test_arr.append(acc_test)\n",
    "                    print(\"Testing ~> Accuracy with {} loss and c_val {}: \\n{}\".format(penalty,c_val,acc_test))\n",
    "                    \n",
    "                \n",
    "                except Warning as w:\n",
    "                    print(w)\n",
    "                    \n",
    "        plt.plot(v,acc_test_arr)\n",
    "        plt.xlabel('C values')\n",
    "        plt.ylabel('Test accuracy')\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training() ~> Accuracy with L1 loss and c_val 0.1 : \n",
      "0.6593406593406593\n",
      "Testing ~> Accuracy with L1 loss and c_val 0.1: \n",
      "0.717391304347826\n",
      "Training() ~> Accuracy with L1 loss and c_val 1 : \n",
      "0.6593406593406593\n",
      "Testing ~> Accuracy with L1 loss and c_val 1: \n",
      "0.717391304347826\n",
      "Training() ~> Accuracy with L1 loss and c_val 10 : \n",
      "0.6593406593406593\n",
      "Testing ~> Accuracy with L1 loss and c_val 10: \n",
      "0.717391304347826\n",
      "Training() ~> Accuracy with L1 loss and c_val 100 : \n",
      "0.6593406593406593\n",
      "Testing ~> Accuracy with L1 loss and c_val 100: \n",
      "0.717391304347826\n",
      "Training() ~> Accuracy with L2 loss and c_val 0.1 : \n",
      "0.9340659340659341\n",
      "Testing ~> Accuracy with L2 loss and c_val 0.1: \n",
      "0.7934782608695652\n",
      "Training() ~> Accuracy with L2 loss and c_val 1 : \n",
      "1.0\n",
      "Testing ~> Accuracy with L2 loss and c_val 1: \n",
      "1.0\n",
      "Training() ~> Accuracy with L2 loss and c_val 10 : \n",
      "1.0\n",
      "Testing ~> Accuracy with L2 loss and c_val 10: \n",
      "1.0\n",
      "Training() ~> Accuracy with L2 loss and c_val 100 : \n",
      "1.0\n",
      "Testing ~> Accuracy with L2 loss and c_val 100: \n",
      "1.0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1114436a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_logistic_model_with(X_train_split,Y_train_split,X_test_split,Y_test_split,params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# let's try support vector machines!\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_linear(c,X_train,Y_train,X_test,Y_test):\n",
    "    '''\n",
    "            Author: Kyle Ong\n",
    "            Date: 05/13/2018\n",
    "            \n",
    "            fits a svm.SVC(probability = False, kernel = 'linear', C = c) to X_train and Y_train\n",
    "            calculates the training and test accuracies \n",
    "            \n",
    "            c: type: int\n",
    "            X_train: numpy.ndarray\n",
    "            Y_train: numpy.ndarray\n",
    "            X_test: numpy.ndarray\n",
    "            Y_test: numpy.ndarray\n",
    "            \n",
    "            will ignore convergence warnings thrown by sklearn\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    linear = svm.SVC(probability=False, kernel='linear',C=c)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore', category=ConvergenceWarning)\n",
    "        \n",
    "        try:\n",
    "\n",
    "            linear.fit(X_train,Y_train)\n",
    "            \n",
    "            Y_hat_train = linear.predict(X_train)\n",
    "            train_acc =np.mean(Y_hat_train == Y_train)\n",
    "            print(\"Training ~> This is the training accuracy with Linear kernel and c_val {}: {}\".format(c,train_acc))\n",
    "            \n",
    "            \n",
    "            Y_hat_test = linear.predict(X_test)\n",
    "            test_acc = np.mean(Y_hat_test == Y_test)\n",
    "            linear_test_acc.append(test_acc)\n",
    "            print(\"Testing ~> This is the testing accuracy with Linear kernel and c_val {}:  {}\".format(c,test_acc))\n",
    "        \n",
    "        except Warning as w:\n",
    "            print(w)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "c_vals = [0.1,1,10,100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ~> This is the training accuracy with Linear kernel and c_val 0.1: 1.0\n",
      "Testing ~> This is the testing accuracy with Linear kernel and c_val 0.1:  1.0\n",
      "Training ~> This is the training accuracy with Linear kernel and c_val 1: 1.0\n",
      "Testing ~> This is the testing accuracy with Linear kernel and c_val 1:  1.0\n",
      "Training ~> This is the training accuracy with Linear kernel and c_val 10: 1.0\n",
      "Testing ~> This is the testing accuracy with Linear kernel and c_val 10:  1.0\n",
      "Training ~> This is the training accuracy with Linear kernel and c_val 100: 1.0\n",
      "Testing ~> This is the testing accuracy with Linear kernel and c_val 100:  1.0\n"
     ]
    }
   ],
   "source": [
    "linear_test_acc = []\n",
    "for c in c_vals:\n",
    "    svm_linear(c,X_train_split,Y_train_split,X_test_split,Y_test_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'linear kernel test accuracies')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nthUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10a90a710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(c_vals,linear_test_acc)\n",
    "plt.xlabel(\"C values\")\n",
    "plt.ylabel(\"linear kernel test accuracies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def svm_rbf(c,X_train,Y_train,X_test,Y_test,gamma):\n",
    "    '''\n",
    "        Author: Kyle Ong\n",
    "        Date: 05/13/2018\n",
    "        \n",
    "        will fit a svm.SVC(probability = False, kernel = 'rbf', gamma = gamma) to X_train and Y_train\n",
    "        will calculate the train accuracy\n",
    "        will calcuate the test accuracy\n",
    "        \n",
    "        c: type: int\n",
    "        X_train: numpy.ndarray\n",
    "        Y_train: numpy.ndarray\n",
    "        X_test: numpy.ndarray\n",
    "        Y_test: numpy.ndarray\n",
    "        gamma: int\n",
    "        \n",
    "        will ignore convergence warnings thrown by sklearn    \n",
    "    '''\n",
    "    \n",
    "    svm_rbf = svm.SVC(probability=False, kernel='rbf',gamma=gamma)\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.filterwarnings('ignore',category=ConvergenceWarning)\n",
    "        \n",
    "        try:\n",
    "            svm_rbf.fit(X_train,Y_train)\n",
    "            \n",
    "            Y_hat_train = svm_rbf.predict(X_train)\n",
    "            train_acc = np.mean(Y_hat_train == Y_train)\n",
    "            print(\"This is the trainning accuracy with  Radial Basis Kernel and c_val {} and gamma {} : {}\".format(c,gamma,train_acc))\n",
    "            \n",
    "            Y_hat_test = svm_rbf.predict(X_test)\n",
    "            test_acc = np.mean(Y_hat_test == Y_test)\n",
    "            print(\"This is the testing accuracy with  Radial Basis Kernel and c_val {} and gamma {} : {}\".format(c,gamma,test_acc))\n",
    "            rbf_test_acc.append(test_acc)\n",
    "            \n",
    "        except Warning as w:\n",
    "            print(w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    0.1: [0.1,1,10,100],\n",
    "    1 :  [0.1,1,10,100],\n",
    "    10 :  [0.1,1,10,100],\n",
    "    100 :  [0.1,1,10,100]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 0.1 and gamma 0.1 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 0.1 and gamma 0.1 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 0.1 and gamma 1 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 0.1 and gamma 1 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 0.1 and gamma 10 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 0.1 and gamma 10 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 0.1 and gamma 100 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 0.1 and gamma 100 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 1 and gamma 0.1 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 1 and gamma 0.1 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 1 and gamma 1 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 1 and gamma 1 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 1 and gamma 10 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 1 and gamma 10 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 1 and gamma 100 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 1 and gamma 100 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 10 and gamma 0.1 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 10 and gamma 0.1 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 10 and gamma 1 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 10 and gamma 1 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 10 and gamma 10 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 10 and gamma 10 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 10 and gamma 100 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 10 and gamma 100 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 100 and gamma 0.1 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 100 and gamma 0.1 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 100 and gamma 1 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 100 and gamma 1 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 100 and gamma 10 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 100 and gamma 10 : 0.717391304347826\n",
      "This is the trainning accuracy with  Radial Basis Kernel and c_val 100 and gamma 100 : 1.0\n",
      "This is the testing accuracy with  Radial Basis Kernel and c_val 100 and gamma 100 : 0.717391304347826\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYsAAAEKCAYAAADjDHn2AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npE6GhSSp0/8HGalzZ9gluqoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10aa06dd8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for k,v in params.items():\n",
    "    \n",
    "    rbf_test_acc = []\n",
    "    c_val = k\n",
    "    \n",
    "    for gamma in v:\n",
    "        svm_rbf(c_val,X_train_split,Y_train_split,X_test_split,Y_test_split,gamma)\n",
    "        \n",
    "    plt.plot(v,rbf_test_acc)\n",
    "    plt.xlabel(\"C Value\")\n",
    "    plt.ylabel(\"Test accuracies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "svm_linear = svm.SVC(probability=False,kernel='linear',C=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=100, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_linear.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = svm_linear.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.savetxt(\"submission.csv\",y_hat,delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.       1.       1.     ...   1.       0.      71.2833]\n",
      " [  4.       1.       1.     ...   1.       0.      53.1   ]\n",
      " [  7.       0.       1.     ...   0.       0.      51.8625]\n",
      " ...\n",
      " [880.       1.       1.     ...   0.       1.      83.1583]\n",
      " [888.       1.       1.     ...   0.       0.      30.    ]\n",
      " [890.       1.       1.     ...   0.       0.      30.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.       1.       1.     ...   1.       0.      71.2833]\n",
      " [  4.       1.       1.     ...   1.       0.      53.1   ]\n",
      " [  7.       0.       1.     ...   0.       0.      51.8625]\n",
      " ...\n",
      " [880.       1.       1.     ...   0.       1.      83.1583]\n",
      " [888.       1.       1.     ...   0.       0.      30.    ]\n",
      " [890.       1.       1.     ...   0.       0.      30.    ]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
